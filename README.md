# ğŸ§  RAG-based Generative AI System

A **Retrieval-Augmented Generation (RAG)** application that combines the power of **Large Language Models (LLMs)** with **vector-based document retrieval** to deliver accurate, context-aware, and verifiable responses.

This project enables users to query their **private or domain-specific data** using natural language â€” making it ideal for **chatbots, document assistants, and knowledge-driven systems**.

---

## ğŸš€ Features

- ğŸ” **Context Retrieval:** Fetches the most relevant chunks from documents using embeddings.  
- ğŸ§© **Generative Response:** Uses an LLM (Llama) to generate grounded answers.  
- ğŸ’¾ **Vector Database Integration:** Supports FAISS / Chroma / Pinecone for similarity search.  
- ğŸ—‚ï¸ **Multi-format Document Support:** Handles PDFs, text files, and more.  
- ğŸ§  **Memory & Chat History:** Retains conversation context.  
- âš™ï¸ **Purely local: doesnt run on any API using llama cpp python model to generate answers.

---

## ğŸ—ï¸ System Architecture

```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚      User Query         â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                Natural Language Input
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Embedding Model       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                 Similarity Search in
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Vector Database    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Retrieved context
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     LLM (Generator)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Final Answer  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Tech Stack

| Component | Technology |
|------------|-------------|
| **Frontend (optional)** | React.js / Streamlit |
| **Backend** | FastAPI / Flask |
| **LLM** | cpp Llama 3 |
| **Embeddings** | OpenAI Embeddings / SentenceTransformers |
| **Vector Store** | FAISS / Chroma / Pinecone |
| **Storage** | Local Files / Cloud Bucket |
| **Deployment** | Docker / AWS / Vercel / Streamlit Cloud |

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/rag-genai-project.git
cd rag-genai-project
```




```

### 5ï¸âƒ£ Run the Application
```bash
docker run --rm -it -p 5001:5000 -v ./data:/app/data cyberdocai
```
will run on localhost 5000



---

## ğŸ“š Example Usage

**Query:**
> â€œSummarize the main findings of the companyâ€™s 2024 annual report.â€

**RAG Pipeline Output:**
> â€œAccording to the 2024 Annual Report, the company saw a 23% growth in digital sales and a 12% increase in overall revenue, primarily driven by e-commerce and subscription-based models.â€

---



---



---

## ğŸ¤ Contributing

Contributions are welcome!  
To contribute:
1. Fork this repository  
2. Create a feature branch (`git checkout -b feature-name`)  
3. Commit changes (`git commit -m "Added new feature"`)  
4. Push to your branch (`git push origin feature-name`)  
5. Open a pull request ğŸ‰  

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” feel free to use, modify, and distribute with attribution.

---

